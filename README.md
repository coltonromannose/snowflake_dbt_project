Project Overview
This project demonstrates how to build a scalable, cloud-based data pipeline using AWS Glue, Amazon S3, dbt, and Snowflake. We extract data from an external API using AWS Glue, store it in S3, and then use dbt to load, model, and transform the data within Snowflake. The pipeline follows a structured multi-layer architecture—raw, transform, and mart—to ensure clean, reliable, and analysis-ready datasets.

Key Features
Automated data extraction via AWS Glue jobs

Cloud storage using Amazon S3

Seamless data loading and transformation with dbt

Data modeling in Snowflake across raw, transform, and mart layers

Secure integration between AWS and Snowflake

Hands-on experience with modern ETL best practices

Workflow Summary
Configure IAM roles for secure AWS Glue and Snowflake access

Extract API data with AWS Glue and store it in S3

Set up Snowflake storage integration to access S3 data

Use dbt Cloud to create and manage data models

Build and deploy raw → transform → mart layers in Snowflake


### Resources:
- Learn more about dbt [in the docs](https://docs.getdbt.com/docs/introduction)
- Check out [Discourse](https://discourse.getdbt.com/) for commonly asked questions and answers
- Join the [dbt community](https://getdbt.com/community) to learn from other analytics engineers
- Find [dbt events](https://events.getdbt.com) near you
- Check out [the blog](https://blog.getdbt.com/) for the latest news on dbt's development and best practices
